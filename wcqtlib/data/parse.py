"""Utilities for wranling the collections, once localized.

The three datasets are assumed to come down off the wire in the following
representations.

# RWC
If obtained from AIST, the data will live on 12 CDs. Here, they have been
backed into a folder hierarchy like the following:

    {base_dir}/
        RWC_I_01/
            {num}/
                {num}{instrument}{style}{dynamic}.{fext}
        RWC_I_02
            ...
        RWC_I_12

Where...
 * base_dir : The path where the data are collated
 * num : A three-digit number contained in the folder
 * instrument : A two-character instrument code
 * style : A two-character style code
 * dynamic : A one-character loudness value

Here, these composite filenames are *ALL* comprised of 8 characters in length.

# Dataset Index
The columns could look like the following:
 * index / id : a unique identifier for each row
 * audio_file :
 * feature_file :
 * dataset :
 * ...?
"""

import argparse
import glob
import hashlib
import json
import logging
import os
import pandas
import re
import sys

import wcqtlib.config as C
import wcqtlib.common.utils as utils

if not hasattr(os, 'scandir'):
    import scandir
    os.scandir = scandir.scandir

logger = logging.getLogger(__name__)

DATA_DIR = os.path.join(
    os.path.dirname(__file__), os.pardir, os.pardir,
    "data")
CANONICAL_FILES_PATH = os.path.join(DATA_DIR, "canonical_files.json")
RWC_INSTRUMENT_MAP_PATH = os.path.join(DATA_DIR, "rwc_instrument_map.json")
CLASS_MAP = os.path.join(DATA_DIR, "class_map.json")
with open(RWC_INSTRUMENT_MAP_PATH, 'r') as fh:
    RWC_INSTRUMENT_MAP = json.load(fh)


def to_basename_df(df):
    """Get a dataframe from df with columns for only [basename, dirname, dataset]
    """
    new_df = df.copy()[['audio_file', 'dataset']]
    dirnames = pandas.DataFrame(columns=['dirname'], index=new_df.index)
    for index, row in new_df.iterrows():
        dirnames.loc[index]['dirname'] = os.path.basename(
            os.path.dirname(row['audio_file']))
        new_df.loc[index]['audio_file'] = os.path.basename(row['audio_file'])
    return pandas.concat([new_df, dirnames], axis=1)


def generate_canonical_files(datasets_df,
                             destination_path=CANONICAL_FILES_PATH):
    """Create the data/canonical_files.json from your existing
    datasets dataframe.

    Parameters
    ----------
    datasets_df : pandas.DataFrame
        DataFrame with columns ["audio_file", "dataset"]

    destination_path : str or None
        Path override for the output. By default,
        goes to 'data/canonical_files.json'

    Returns
    -------
    success : bool
        True if file was created.
    """
    canonical_df = to_basename_df(datasets_df)
    canonical_df.to_json(destination_path, orient='records')

    return os.path.exists(destination_path)


def load_canonical_files(destination_path=CANONICAL_FILES_PATH):
    """Read the canonical files dataframe."""
    return pandas.read_json(destination_path, orient='records')


def diff_datasets_files(canonical_df, datasets_df):
    """Compare two dataframes' audio_file basenames, and return any
    rows in canonical_df which are not in datasets_df.

    Parameters
    ----------
    canonical_df : pandas.DataFrame
        Columns: ["audio_file", "dirname", "dataset"]
            Where audio_file is the basename of the audiofile.

    datasets_df : pandas.DataFrame
        The datasets_df generated by parse_files_to_dataframe.

    Returns
    -------
    diff_df : pandas.DataFrame
        Dataframe with rows for only the missing entries.
    """
    canonical_files = canonical_df.set_index('audio_file')
    datasets_files = to_basename_df(datasets_df).set_index('audio_file')

    # Print some stats with the logger.
    logger.info("Dataset Stats")
    logger.info(utils.colored("--Canonical--", "green"))
    logger.info("RWC: {}".format(
        len(canonical_files[canonical_files["dataset"] == "rwc"])))
    logger.info("uiowa: {}".format(
        len(canonical_files[canonical_files["dataset"] == "uiowa"])))
    logger.info("philharmonia: {}".format(
        len(canonical_files[canonical_files["dataset"] == "philharmonia"])))

    logger.info(utils.colored("--Yours--", "green"))
    logger.info("RWC: {}".format(
        len(datasets_files[datasets_files["dataset"] == "rwc"])))
    logger.info("uiowa: {}".format(
        len(datasets_files[datasets_files["dataset"] == "uiowa"])))
    logger.info("philharmonia: {}".format(
        len(datasets_files[datasets_files["dataset"] == "philharmonia"])))

    joinresult = canonical_files.join(
        datasets_files, how='outer', lsuffix='can', rsuffix='me', sort=True)

    nulls = pandas.isnull(joinresult).any(1).nonzero()[0]

    return joinresult.iloc[nulls]


def rwc_instrument_code_to_name(rwc_instrument_code):
    """Use the rwc_instrument_map.json to convert an rwc_instrument_code
    to it's instrument name.

    Parameters
    ----------
    rwc_instrument_code : str
        Two character instrument code

    Returns
    -------
    instrument_name : str
        Full instrument name, if it exists, else the code.
    """
    instrument_name = RWC_INSTRUMENT_MAP.get(
        rwc_instrument_code, rwc_instrument_code)
    return instrument_name if instrument_name else rwc_instrument_code


def parse_rwc_path(rwc_path):
    """Takes an rwc path, and returns the extracted codes from the
    filename.

    Parameters
    ----------
    rwc_path : str
        Full path or basename. If full path, gets the basename.

    Returns
    -------
    instrument_name : str
    style_code : str
    dynamic_code : str
    """
    filebase = utils.filebase(rwc_path)
    instrument_code = filebase[3:5]
    # Get the instrument name from the json file.
    instrument_name = rwc_instrument_code_to_name(instrument_code)
    style_code = filebase[5:7]
    dynamic_code = filebase[7]
    return instrument_name, style_code, dynamic_code


def generate_id(dataset, audio_file_path):
    """Create a unique identifier for this entry.

    Returns
    -------
    id : str
        dataset[0] + md5(audio_file_path)[:8]
    """
    dataset_code = dataset[0]
    audio_file_hash = hashlib.md5(
        utils.filebase(audio_file_path)
        .encode('utf-8')).hexdigest()
    return "{0}{1}".format(dataset_code, audio_file_hash[:8])


def rwc_to_dataframe(base_dir, dataset="rwc"):
    """Convert a base directory of RWC files to a pandas dataframe.

    Parameters
    ----------
    base_dir : str
        Full path to the base RWC directory.

    dataset : str
        Datset string to use in this df.

    Returns
    -------
    pandas.DataFrame
        Indexed by:
            id : [dataset identifier] + [8 char md5 of filename]
        With the following columns:
            audio_file : full path to original audio file.
            dataset : dataset it is from
            instrument : instrument label.
            dynamic : dynamic tag
    """
    logger.info("Scanning RWC directory for audio files.")

    indexes = []
    records = []
    for audio_file_path in glob.glob(os.path.join(base_dir, "*/*/*.flac")):
        instrument_name, style_code, dynamic_code = \
            parse_rwc_path(audio_file_path)

        indexes.append(generate_id(dataset, audio_file_path))
        records.append(
            dict(audio_file=audio_file_path,
                 dataset=dataset,
                 instrument=instrument_name,
                 dynamic=dynamic_code))

    logger.info("Using {} files from RWC.".format(len(records)))

    return pandas.DataFrame(records, index=indexes)


def parse_uiowa_path(uiowa_path):
    filename = utils.filebase(uiowa_path)
    parameters = [x.strip() for x in filename.split('.')]
    instrument = parameters.pop(0)
    # This regex matches note names with a preceeding and following '.'
    note_match = re.search(r"(?<=\.)[A-Fb#0-6]*(?<!\.)", filename)
    notevalue = filename[note_match.start():note_match.end()] \
        if note_match else None
    # This regex matches dynamic chars with a preceeding and following '.'
    dynamic_match = re.search(r"(?<=\.)[f|p|m]*(?<!\.)", filename)
    dynamic = filename[dynamic_match.start():dynamic_match.end()] \
        if dynamic_match else None
    return instrument, dynamic, notevalue


def get_num_notes_from_uiowa_filename(uiowa_path):
    """Return the expected number of notes in a UIowa filename if it can be
    determined.

    E.g.
    Bb1B1 => 2
    C4B4 => 12
    C5Bb5 => 10
    B3 => 1
    booger => None

    Parameters
    ----------
    filename : str
        A filename potentially conforming to the UIowa MIS conventions.

    Returns
    -------
    num_notes : int, or None
        Number of notes expected to be in the file, or None if this
        information cannot be confidently inferred.
    """
    instrument, dynamic, notevalue = parse_uiowa_path(uiowa_path)
    result = None

    if notevalue:
        notes = re.findall(r"([A-F][b#]?[0-6])", notevalue)
        if len(notes) == 1:
            result = 1
        elif len(notes) == 2:
            # Get note distance
            result = 1 + get_note_distance(notes)

    return result


def get_note_distance(note_pair):
    """Get the distance in semitones between two named
    notes.

    E.g.
    (Bb1, B1) => 1
    (C4, B4) => 11
    (C5, Bb5) => 10

    Parameters
    ----------
    note_pair : tuple of ints

    Returns
    -------
    note_distance : int
    """
    char_map = {'C': 0, 'D': 2, 'E': 4, 'F': 5, 'G': 7, 'A': 9, 'B': 11}

    def apply_modifier(val, modifier):
        if modifier == "#":
            return val + 1
        elif modifier == 'b':
            return val - 1
        else:
            return val

    # get the chars
    first_note = [x for x in note_pair[0]]
    second_note = [x for x in note_pair[1]]

    first_name, first_oct = first_note[0], first_note[-1]
    first_mod = first_note[1] if len(first_note) == 3 else None
    second_name, second_oct = second_note[0], second_note[-1]
    second_mod = second_note[1] if len(second_note) == 3 else None

    base_dist = apply_modifier(char_map[second_name], second_mod) - \
        apply_modifier(char_map[first_name], first_mod)

    oct_diff = int(second_oct) - int(first_oct)
    base_dist += (oct_diff * 12)

    return base_dist


def uiowa_to_dataframe(base_dir, dataset="uiowa"):
    """Convert a base directory of UIowa files to a pandas dataframe.

    Parameters
    ----------
    base_dir : str
        Full path to the base RWC directory.

    dataset : str
        Datset string to use in this df.

    Returns
    -------
    pandas.DataFrame
        With the following columns:
            id
            audio_file
            dataset
            instrument
            dynamic
            note
            parent : instrument category.
    """
    logger.info("Scanning UIOWA directory for audio files.")

    indexes = []
    records = []
    root_dir = os.path.join(base_dir, "theremin.music.uiowa.edu",
                            "sound files", "MIS")
    for item in os.scandir(root_dir):
        if item.is_dir():
            parent_cagetegory = item.name
            audio_files = glob.glob(os.path.join(item.path, "*/*.aif*"))
            for audio_file_path in audio_files:
                instrument, dynamic, notevalue = \
                    parse_uiowa_path(audio_file_path)

                indexes.append(generate_id(dataset, audio_file_path))
                records.append(
                    dict(audio_file=audio_file_path,
                         dataset=dataset,
                         instrument=instrument,
                         dynamic=dynamic,
                         note=notevalue,
                         parent=parent_cagetegory))

    logger.info("Using {} files from UIOWA.".format(len(records)))

    return pandas.DataFrame(records, index=indexes)


def parse_phil_path(phil_path):
    """Convert phil path to codes/parameters.

    Parameters
    ----------
    phil_path : full path.

    Returns
    -------
    tuple of parameters.
    """
    audio_file_name = utils.filebase(phil_path)
    instrument, note, duration, dynamic, articulation = \
        audio_file_name.split('_')
    return instrument, note, duration, dynamic, articulation


def philharmonia_to_dataframe(base_dir, dataset="philharmonia"):
    """Convert a base directory of Philharmonia files to a pandas dataframe.

    Parameters
    ----------
    base_dir : str
        Full path to the base RWC directory.

    dataset : str
        Datset string to use in this df.

    Returns
    -------
    pandas.DataFrame
        With the following columns:
            id
            audio_file
            dataset
            instrument
            note
            dynamic
    """
    logger.info("Scanning Philharmonia directory for audio files.")

    root_dir = os.path.join(base_dir, "www.philharmonia.co.uk",
                            "assets/audio/samples")

    # These files come in zips. Extract them as necessary.
    zip_files = glob.glob(os.path.join(root_dir, "*/*.zip"))
    utils.unzip_files(zip_files)

    articulation_skipped = []

    indexes = []
    records = []
    for audio_file_path in glob.glob(os.path.join(root_dir, "*/*/*.mp3")):
        instrument, note, duration, dynamic, articulation = \
            parse_phil_path(audio_file_path)

        if "normal" in articulation or "vibrato" in articulation:
            indexes.append(generate_id(dataset, audio_file_path))
            records.append(
                dict(audio_file=audio_file_path,
                     dataset=dataset,
                     instrument=instrument,
                     note=note,
                     dynamic=dynamic))
        else:
            articulation_skipped += [audio_file_path]

    logger.info("Using {} files from Philharmonia.".format(len(records)))
    logger.warn(utils.colored("Skipped {} files in Philharmonia with "
                              "articulation not in 'normal'"
                              .format(len(articulation_skipped)), "red"))

    with open("log_philharmonia_skipped.txt", 'w') as fh:
        fh.write("\n".join(articulation_skipped))

    return pandas.DataFrame(records, index=indexes)


def normalize_instrument_names(datasets_df):
    """Convert all the varied datasets representation of
    instrument names to the single one used in
    our class set.

    Parameters
    ----------
    datasets_df : pandas.DataFrame with an "instrument" column.

    Returns
    -------
    normalized_df : pandas.DataFrame
        A copy of your dataframe, with instruments only from
        the InstrumentClassMap
    """
    classmap = InstrumentClassMap()
    new_df = datasets_df.copy()
    for i in range(len(new_df)):
        old_class = new_df.iloc[i]["instrument"]
        new_df.iloc[i]["instrument"] = classmap[old_class]
    return new_df


def load_dataframes(data_dir):
    """Load all the datasets into a single dataframe.

    Parameters
    ----------
    data_dir : str

    Returns
    -------
    dataframe : pandas.DataFrame()
        Dataframe containing pointers to all the files.
    """
    rwc_df = rwc_to_dataframe(os.path.join(data_dir, "RWC Instruments"))
    uiowa_df = uiowa_to_dataframe(os.path.join(data_dir, "uiowa"))
    philharmonia_df = philharmonia_to_dataframe(
        os.path.join(data_dir, "philharmonia"))

    result = pandas.concat([rwc_df, uiowa_df, philharmonia_df])
    logger.info("Total dataset records: {}".format(len(result)))

    return result


class InstrumentClassMap(object):
    """Class for handling map between class names and the
    names they possibly could be from the datasets."""

    def __init__(self, file_path=CLASS_MAP):
        """
        Parameters
        ----------
        file_path : str
        """
        with open(file_path, 'r') as fh:
            self.data = json.load(fh)

        # Create the reverse map so we can efficiently do the
        # reverse lookup
        self.reverse_map = {}
        for classname in self.data:
            for item in self.data[classname]:
                self.reverse_map[item] = classname

        self.index_map = {}
        for i, classname in enumerate(sorted(self.data.keys())):
            self.index_map[classname] = i

    @property
    def allnames(self):
        """Return a complete list of all class names for searching the
        dataframe."""
        return sorted(self.reverse_map.keys())

    @property
    def classnames(self):
        return sorted(self.data.keys())

    def __getitem__(self, searchkey):
        """Get the actual class name. (Actually the reverse map)."""
        return self.reverse_map.get(searchkey, None)

    def get_index(self, searchkey):
        """Get the class index for training.

        This is actually the index of the sorted keys.

        Parameters
        ----------
        searchkey : str

        Returns
        -------
        index : int
        """
        return self.index_map[self[searchkey]]

    def from_index(self, index):
        """Get the instrument name for an index."""
        return sorted(self.data.keys())[index]

    @property
    def size(self):
        """Return the size of the index map (the number of
        data keys)
        """
        return len(self.data.keys())


def parse_files_to_dataframe(config):
    """Do-the-thing function for loading all of the
    datasets in and creating a dataframe pointing to all
    of the files and their metadata.

    Results in the creation of the datasets_df
    at the path specified by the config.

    Parameters
    ----------
    config : config.Config
        The config specifying where all the important stuff lives.

    Returns
    -------
    success : bool
        True if succeeded, else False
    """
    # Load the datasets dataframe
    print("Loading dataset...")
    data_dir = os.path.expanduser(config["paths/data_dir"])
    dfs = load_dataframes(data_dir)
    logger.info("Datasets contain {} audio files.".format(len(dfs)))
    # Save it to a json file
    extract_dir = os.path.expanduser(config["paths/extract_dir"])
    utils.create_directory(extract_dir)
    output_path = os.path.join(extract_dir, config["dataframes/datasets"])
    logger.debug("Saving to", output_path)
    dfs.to_json(output_path)
    try:
        df = pandas.read_json(output_path)
        if not df.empty:
            print("Created artifact: {}".format(
                utils.colored(output_path, "cyan")))
            return True
    finally:
        return False


def print_stats(config):
    datasets_path = os.path.join(
        os.path.expanduser(config['paths/extract_dir']),
        config['dataframes/datasets'])
    notes_path = os.path.join(
        os.path.expanduser(config['paths/extract_dir']),
        config['dataframes/notes'])
    features_path = os.path.join(
        os.path.expanduser(config['paths/extract_dir']),
        config['dataframes/features'])
    datasets_df = pandas.read_json(datasets_path) \
        if os.path.exists(datasets_path) else \
        pandas.DataFrame(columns=["dataset", "instrument"])
    notes_df = pandas.read_pickle(notes_path) \
        if os.path.exists(notes_path) else \
        pandas.DataFrame(columns=["dataset", "instrument"])
    features_df = pandas.read_pickle(features_path) \
        if os.path.exists(features_path) else \
        pandas.DataFrame(columns=["dataset", "instrument"])

    print(utils.colored("{:<20} {:<30} {:<30} {:<30}".format(
        "item", "datasets_df", "notes_df", "features_df")))
    print("{:<20} {:<30} {:<30} {:<30}".format(
        "count", len(datasets_df), len(notes_df), len(features_df)))

    datasets = ["rwc", "uiowa", "philharmonia"]

    def print_datasetcount(dataset):
        print("{:<20} {:<30} {:<30} {:<30}".format(
            "{} count".format(dataset),
            len(datasets_df[datasets_df["dataset"] == dataset]),
            len(notes_df[notes_df["dataset"] == dataset]),
            len(features_df[features_df["dataset"] == dataset])))
    for dataset in datasets:
        print_datasetcount(dataset)

    def print_dataset_instcount(df, instrument):
        inst_filter = df[df["instrument"] == instrument]
        print("{:<20} {:<30} {:<30} {:<30}".format(
            "{} count".format(instrument),
            len(inst_filter[inst_filter["dataset"] == "rwc"]),
            len(inst_filter[inst_filter["dataset"] == "uiowa"]),
            len(inst_filter[inst_filter["dataset"] == "philharmonia"])))

    classmap = InstrumentClassMap()

    print("---------------------------")
    print("Datasets-Instrument count / dataset")
    print("---------------------------")
    print(utils.colored("{:<20} {:<30} {:<30} {:<30}".format("item", "rwc", "uiowa", "philharmonia")))
    for inst in sorted(datasets_df["instrument"].unique()):
        if inst in classmap.allnames:
            print_dataset_instcount(datasets_df, inst)

    print("---------------------------")
    print("Notes-Instrument count / dataset")
    print("---------------------------")
    if not notes_df.empty:
        print(utils.colored("{:<20} {:<30} {:<30} {:<30}".format("item", "rwc", "uiowa", "philharmonia")))
        for inst in sorted(notes_df["instrument"].unique()):
            print_dataset_instcount(notes_df, inst)
    else:
        print("no notes_df available")

    print("---------------------------")
    print("Features-Instrument count / dataset")
    print("---------------------------")
    if not features_df.empty:
        print(utils.colored("{:<20} {:<30} {:<30} {:<30}".format("item", "rwc", "uiowa", "philharmonia")))
        for inst in classmap.classnames:
            print_dataset_instcount(features_df, inst)
    else:
        print("No features_df available")


if __name__ == "__main__":
    CONFIG_PATH = os.path.join(os.path.dirname(__file__), os.pardir,
                               os.pardir, "data", "master_config.yaml")
    parser = argparse.ArgumentParser(
        description='Parse raw data into dataframe')
    parser.add_argument("-c", "--config_path", default=CONFIG_PATH)
    args = parser.parse_args()

    logging.basicConfig(level=logging.DEBUG)

    # Load the config
    config = C.Config.from_yaml(args.config_path)
    success = parse_files_to_dataframe(config)
    sys.exit(0 if success else 1)
