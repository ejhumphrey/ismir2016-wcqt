import numpy as np
import pandas
import progressbar

import wcqtlib.data.parse as parse
instrument_map = parse.InstrumentClassMap()


def evaluate_one(dfrecord, model, slicer_fx, t_len):
    """Return an evaluation object/dict after evaluating
    a single model using a loaded model.

    This method runs the model.predict over every set of
    frames generated by slicer_fx, and returns
    the class with the maximum

    Parameters
    ----------
    dfrecord : pandas.DataFrame
        pandas.Series containing the record to evaluate.

    model : models.NetworkManager

    slicer_fx : function
        Function that extracts featuers fr eaach frame
        from the feature file.

    t_len : int

    Returns
    ------
    results : pandas.Series
        All the results stored as a pandas.Series
    """
    # Get predictions for every timestep in the file.
    results = []
    losses = []
    accs = []
    target = instrument_map.get_index(dfrecord["instrument"])

    for frames in slicer_fx(dfrecord, t_len=t_len,
                            shuffle=False, auto_restart=False):
        results += [model.predict(frames)]
        loss, acc = model.evaluate(frames)
        losses += [loss]
        accs += [acc]
    results = np.concatenate(results)
    mean_loss = np.mean(losses)
    mean_acc = np.mean(accs)

    class_predictions = results.argmax(axis=1)

    # calculate the maximum likelyhood class - the class with the highest
    #  predicted probability across all frames.
    max_likelyhood_class = results.max(axis=0).argmax()

    # Also calculate the highest voted frame.
    vote_class = np.asarray(np.bincount(class_predictions).argmax(),
                            dtype=np.int)

    # Return both of these as a dataframe.
    return pandas.Series(
        data=[mean_loss, mean_acc, max_likelyhood_class, vote_class, target],
        index=["mean_loss", "mean_acc", "max_likelyhood", "vote", "target"],
        name=dfrecord.name)


def evaluate_dataframe(test_df, model, slicer_fx, t_len, show_progress=False):
    """Run evaluation on the files in a dataframe.

    Parameters
    ----------
    test_df : pandas.DataFrame
        DataFrame pointing to the features files and targets to evaluate.

    model : models.NetworkManager

    slicer_fx : function
        Function that extracts featuers fr eaach frame
        from the feature file.

    t_len : int

    Returns
    -------
    results_df : pandas.DataFrame
        DataFrame containing the results from for each file,
        where the index of the original file is maintained, but
        the dataframe now contains the columns:
            * max_likelyhood
            * vote
            * target
    """
    results = []
    if show_progress:
        i = 0
        progress = progressbar.ProgressBar(max_value=len(test_df))

    for index, row in test_df.iterrows():
        results += [evaluate_one(row, model, slicer_fx, t_len)]

        if show_progress:
            progress.update(i)
            i += 1

    return pandas.DataFrame(results)


def analyze_results(eval_df, experiment_name=None):
    """
    Parameters
    ----------
    eval_df : pandas.DataFrame
        Dataframe containing the predictions and ground truth for each file.

    Returns
    -------
    analysis : pandas.Series
        Series containing
         * accuracy
    """
    tps = eval_df["max_likelyhood"] == eval_df["target"]
    tp_count = tps.sum()
    accuracy = tps.mean()
    return pandas.Series(
        [tp_count, accuracy],
        index=["tp_count", "accuracy"])
